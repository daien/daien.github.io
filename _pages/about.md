---
permalink: /
title: "Adrien Gaidon, PhD"
excerpt: "About me"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

I lead the Machine Learning Research team at the [Toyota Research Institute (TRI)](https://www.tri.global/) in Los Altos, CA, USA. My research focuses on scaling up ML for Robot Autonomy, spanning Scene and Behavior Understanding, Simulation for Deep Learning, 3D Computer Vision, and Self-Supervised Learning (cf. [a short bio](#bio) below). Outside of work, I spend time with my wife, daughter, and the mountains (I love camping, climbing, and snowboarding).

## News

- August 2021: my team is growing! We are hiring **4 new Research Scientists** in areas including [Perception](https://jobs.lever.co/tri/30b2e353-ca5a-43fc-bdce-08369b6f3bc9), [Reconstruction / Inverse Graphics](https://jobs.lever.co/tri/4ead5bb5-c0b7-4fd8-9603-e63970a7cc77), [Computer Vision Safety](https://jobs.lever.co/tri/fbb46694-f7b9-4582-bbdb-2d650d638ef9), and [RL Safety](https://jobs.lever.co/tri/3072275b-dcd7-4406-963f-0e84f2a61089). We are also looking for [3 interns](https://jobs.lever.co/tri/36c8b791-b9e8-4888-9c31-337cb3dfe04f) on these topics. Please apply if you are interested in joining a talented team on a mission to discover the learning principles for safe robot autonomy and human amplification at scale!
- July 2021: **4 papers accepted at ICCV**! Self-supervised sim-to-real transfer ([GUDA](https://arxiv.org/abs/2103.16694)), Learning to Track with Object Permanence ([PermaTrack](https://arxiv.org/abs/2103.14258)), self-supervised pre-training for monocular 3D object detection ([DD3D](https://arxiv.org/abs/2108.06417)), and video auto-labeling (coming soon).
- July 2021: I gave a talk on [Bridging the Perception-Control gap with Prediction](https://www.youtube.com/watch?v=EmrReoNQP3s&t=4840s) at [RSS'21](https://negarmehr.github.io/RSS2021Workshop/).
- July 2021: I did a [fun interview](https://paralleldomain.com/how-tri-trains-better-computer-vision-models-with-pd-synthetic-data/) with the founder & CEO of [Parallel Domain](https://paralleldomain.com/), Kevin McNamara, on how we use Synthetic Data to advance the state of the art in Computer Vision. PD also made a cool blog covering our recent [PermaTrack](https://arxiv.org/abs/2103.14258) work.
- May 2021: got an **[outstanding reviewer award at CVPR](http://cvpr2021.thecvf.com/node/184)**!
- May 2021: we are organizing the CVPR'21 [DDAD depth estimation challenge](https://eval.ai/web/challenges/challenge-page/902/overview)! Try your ideas on public data from our TRI fleet! Winners will win prizes and present, along with prestigious invited speakers, at our [Mono3D CVPR'21 workshop](https://sites.google.com/view/mono3d-workshop/) on the Frontiers of Monocular 3D Perception.
- May 2021: together with Vitor and Rares, we wrote a blog post summarizing a lot of our research results in [self-supervised learning for depth estimation](https://medium.com/toyotaresearch/self-supervised-learning-in-depth-part-1-of-2-74825baaaa04).
- May 2021: I did a [fun interview about ML for Autonomous Driving](https://wandb.ai/wandb_fc/gradient-dissent/reports/TRI-s-Adrien-Gaidon-on-advancing-ML-research-in-autonomous-vehicles--Vmlldzo2MzEzMTE) with Lukas Biewald for the Gradient Dissent podcast. Check it out!
- April 2021: new preprints on self-supervised sim2real transfer ([GUDA](https://arxiv.org/abs/2103.16694)), Full Surround Monodepth ([FSM](https://arxiv.org/abs/2104.00152)), Learning to Track with Object Permanence ([PermaTrack](https://arxiv.org/abs/2103.14258)), and trajectory forecasting ([HAICU](https://arxiv.org/abs/2104.12446)).
- March 2021: 2 multi-task learning papers accepted at [CVPR'21](http://cvpr2021.thecvf.com/), one on joint depth prediction and completion ([PackNet-SAN](https://arxiv.org/abs/2103.16690)) and another on [Hierarchical Lov√°sz Embeddings for panoptic segmentation](https://arxiv.org/abs/2106.04555).
- March 2021: Blake Wulfe and I **won the [NeurIPS 2020 ProcGen RL Competition](https://www.aicrowd.com/challenges/neurips-2020-procgen-competition)**! Find the full report [here](https://arxiv.org/abs/2103.15332).
- February 2021: 1 paper at RoboSoft in collaboration with the brilliant TRI Robotics team on [monocular depth estimation inside visuotactile sensors](/publication/2021-01-22-Monocular-Depth-SoftBubble).
- January 2021: 1 paper on [distributionally robust control](/publication/2021-01-30-RAT-iLQR) accepted at RA-L/ICRA and another at [ICLR](https://iclr.cc/) on [regularization for heteroskedastic and imbalanced Deep Learning](/publication/2021-01-12-Heteroskedastic-and-Imbalanced-Deep-Learning). Both on robustness and with great Stanford collaborators from [Mac Schwager](https://web.stanford.edu/~schwager/) and [Tengyu Ma](https://ai.stanford.edu/~tengyuma/)'s labs.

### 2020 Updates

- November 2020: 1 paper (**oral**) at [3DV](http://3dv2020.dgcv.nii.ac.jp/) on [Neural Ray Surfaces](/publication/2020-08-15-Neural-Ray-Surfaces) in collaboration with [Greg Shakhnarovich](https://ttic.uchicago.edu/~gregory/) at TTIC.
- October 2020: got a "top 10% reviewer award" at NeurIPS!
- October 2020: 2 papers accepted at [CoRL 2020](https://www.robot-learning.org/), including one **[oral on self-supervised 3D keypoints](/publication/2020-11-16-KP3D-Self-supervised-3D-Kepyoints)** and a [poster on interpretable trajectory forecasting](/publication/2020-11-16-MATS-Trajectory-Forecasting) in collaboration with [Marco Pavone](https://web.stanford.edu/~pavone/)'s lab at Stanford
- October 2020: I gave an [invited talk at IPAM](https://www.youtube.com/watch?v=HltMaglvFKg) covering a lot of our recent results across the full AV stack.
- August 2020: we are organizing the [ECCV 2020 workshop on Perception for Autonomous Driving (PAD)](https://sites.google.com/view/pad2020)
- July 2020: we are organizing the [ICML 2020 workshop on AI for Autonomous Driving (AIAD)](https://sites.google.com/view/aiad2020)
- July 2020: 2 papers accepted at [ECCV 2020](https://eccv2020.eu/), including an **[oral on trajectory prediction](/publication/2020-08-23-Endpoint-Conditioned-Trajectory-Prediction)** and a [poster on differentiable rendering](/publication/2020-08-23-Self-Supervised-Differentiable-Rendering)
- July 2020: [5 papers](/publications/) accepted at [IROS 2020](https://www.iros2020.org/) and 1 at [ITSC](https://www.ieee-itsc2020.org/) on [scene flow](/publication/2020-10-25-End-to-end-Birds-eye-view-Flow), [imitation learning](/publication/2020-10-25-Driving-Through-Ghosts), [game-theoretic planning](/publication/2020-10-25-Game-Theoretic-Planning-Risk-Aware), [risk sensitive control](/publication/2020-10-25-Risk-Sensitive-Control-Trajectron), [traffic simulation](/publication/2020-10-25-Behaviorally-Diverse-Traffic-Simulation-via-RL), and [planner testing](/publication/2020-09-20-Discovering-Avoidable-Planner-Failures).
- July 2020: [1 paper with Stanford on hierarchical RL and imitation in near-accidents](/publication/2020-07-15-Reinforcement-Learning-based-Control-of-Imitative-Policies-for-Near-Accident-Driving) accepted at [RSS 2020](https://roboticsconference.org/)
- June 2020: Together with colleagues from PFN and TRI-AD, we have released a [survey on Differentiable Rendering](/publication/2020-06-22-Differentiable-Rendering-Survey).
- June 2020: 4 papers (**3 orals!**) accepted at [CVPR 2020](http://cvpr2020.thecvf.com/) ([PackNet pseudo-lidar](/publication/2020-06-16-3D-Packing-for-Self-Supervised-Monocular-Depth-Estimation), [real-time panoptic segmentation](/publication/2020-06-16-Real-Time-Panoptic-Segmentation-from-Dense-Detections), [auto-labeling via Differentiable Rendering](/publication/2020-06-16-Autolabeling-3D-Objects-with-Differentiable-Rendering), [spatio-temporal graph distillation](/publication/2020-06-16-Spatio-Temporal-Graph-for-Video-Captioning)). See also our new [DDAD dataset](https://github.com/TRI-ML/DDAD) for depth estimation!
- February 2020: [1 paper with Stanford on Pedestrian Intent Prediction](/publication/2020-05-31-Spatiotemporal-Relationship-Reasoning-for-Pedestrian-Intent-Prediction) accepted at [RA-L](https://www.ieee-ras.org/publications/ra-l)/[ICRA 2020](https://www.icra2020.org/). See also our new [STIP dataset](https://stip.stanford.edu/dataset.html)!
- January 2020: I got promoted to Senior Manager! Super grateful to my team and excited for our next steps together!

### Older updates

- December 2019: 1 paper accepted at [ICLR 2020](https://iclr.cc/virtual/poster_ByxT7TNFvH.html) and 1 (**oral**) at [WACV 2020](http://wacv20.wacv.net/)
- October 2019: 1 paper accepted at the [International Journal of Computer Vision (IJCV)](https://link.springer.com/article/10.1007/s11263-019-01222-z)
- September 2019: 1 paper accepted at [NeurIPS 2019](https://nips.cc/Conferences/2019/Schedule?showEvent=13370) (also oral at [BayLearn 2019](https://www.youtube.com/watch?v=EIF6Sy3ZKYQ&feature=emb_logo)) and 2 papers (**spotlights**) accepted at [CoRL 2019](https://sites.google.com/robot-learning.org/corl2019)
- July 2019: 1 paper accepted (**oral**) at [ICCV 2019](https://conftube.com/video/2ntDYowHbZs?tocitem=134)
- May 2019: I did an interview with the wonderful Sam Charrington for the [TWIML AI podcast](https://twimlai.com/twiml-talk-269-advancing-autonomous-vehicle-development-using-distributed-deep-learning-with-adrien-gaidon/)!


## Bio

Adrien Gaidon is the Head of Machine Learning Research at the [Toyota Research Institute (TRI)](https://www.tri.global/) in Los Altos, CA, USA. Adrien's research focuses on scaling up ML for robot autonomy, spanning Scene and Behavior Understanding, Simulation for Deep Learning, 3D Computer Vision, and Self-Supervised Learning. He received his PhD from [Microsoft Research - Inria](https://www.msr-inria.fr/) Paris in 2012, has over 50 publications and patents in ML & Computer Vision (cf. [Google Scholar](https://scholar.google.com/citations?user=2StUgf4AAAAJ&hl=en)), and his research is used in a variety of domains, including automated driving. You can find him at [adriengaidon.com](https://www.adriengaidon.com), on [linkedin](https://www.linkedin.com/in/adrien-gaidon-63ab2358/), and Twitter [@adnothing](https://twitter.com/adnothing).


